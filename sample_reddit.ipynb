{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample_reddit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmQP2iVVJ3kljX1w9hIWmu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gail529/NLP/blob/master/sample_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l73B-QWmuB8C"
      },
      "source": [
        "# General purpose script for using PRAW to scrape REDDIT data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soqp5olS6kbu",
        "outputId": "141f403e-e12c-40f7-be7b-96adf4ef917a"
      },
      "source": [
        "pip install asyncpraw==7.2.0\n",
        "!pip install praw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asyncpraw==7.2.0 in /usr/local/lib/python3.7/dist-packages (7.2.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from asyncpraw==7.2.0) (0.18.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.7/dist-packages (from asyncpraw==7.2.0) (0.7.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2 in /usr/local/lib/python3.7/dist-packages (from asyncpraw==7.2.0) (2.0.1)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.18->asyncpraw==7.2.0) (2.23.0)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from asyncprawcore<3,>=2->asyncpraw==7.2.0) (1.6.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from asyncprawcore<3,>=2->asyncpraw==7.2.0) (3.7.4.post0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw==7.2.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw==7.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw==7.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw==7.2.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->asyncprawcore<3,>=2->asyncpraw==7.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->asyncprawcore<3,>=2->asyncpraw==7.2.0) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->asyncprawcore<3,>=2->asyncpraw==7.2.0) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->asyncprawcore<3,>=2->asyncpraw==7.2.0) (3.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwjh5a86ZgB"
      },
      "source": [
        "# Scraping a single subreddit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ewYjtYD6ZJQ"
      },
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "reddit = praw.Reddit(client_id='#', \\\n",
        "                     client_secret='#', \\\n",
        "                     user_agent='#', \\\n",
        "                     username='#', \\\n",
        "                     password='#')\n",
        "\n",
        "topics_dict = {\n",
        "    \"title\":[], \n",
        "    \"score\":[], \n",
        "    \"id\":[],\n",
        "    \"url\":[], \n",
        "    \"comms_num\": [], \n",
        "    \"created\": [], \n",
        "    \"body\":[]\n",
        "    }\n",
        "\n",
        "\n",
        "subred='your_subreddit'\n",
        "subreddit = reddit.subreddit(subred)\n",
        "top_subreddit = subreddit.hot(limit=500)\n",
        "\n",
        "for submission in top_subreddit:\n",
        "    topics_dict[\"title\"].append(submission.title)\n",
        "    topics_dict[\"score\"].append(submission.score)\n",
        "    topics_dict[\"id\"].append(submission.id)\n",
        "    topics_dict[\"url\"].append(submission.url)\n",
        "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
        "    topics_dict[\"created\"].append(submission.created)\n",
        "    topics_dict[\"body\"].append(submission.selftext)\n",
        "\n",
        "topics_data = pd.DataFrame(topics_dict)\n",
        "topics_data\n",
        "topics_data.to_csv('reddit_Df.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjnb3Qsm59BX"
      },
      "source": [
        "# Scraping several subreddits at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwO57Pt5m8E"
      },
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "reddit = praw.Reddit(client_id='#', \\\n",
        "                     client_secret='#', \\\n",
        "                     user_agent='#', \\\n",
        "                     username='#', \\\n",
        "                     password='#')\n",
        "\n",
        "\n",
        "subreddit_list=['subreddit_1',\n",
        "                'subreddit_2',\n",
        "               ]\n",
        "\n",
        "topics_dict = {\n",
        "    \"title\":[], \n",
        "    \"score\":[], \n",
        "    \"id\":[],\n",
        "    \"url\":[], \n",
        "    \"comms_num\": [], \n",
        "    \"created\": [], \n",
        "    \"body\":[]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "for subred in subreddit_list: \n",
        "    subreddit = reddit.subreddit(subred)\n",
        "\n",
        "    top_subreddit = subreddit.top(limit=500)\n",
        "    for submission in top_subreddit:\n",
        "        topics_dict[\"title\"].append(submission.title)\n",
        "        topics_dict[\"score\"].append(submission.score)\n",
        "        topics_dict[\"id\"].append(submission.id)\n",
        "        topics_dict[\"url\"].append(submission.url)\n",
        "        topics_dict[\"comms_num\"].append(submission.num_comments)\n",
        "        topics_dict[\"created\"].append(submission.created)\n",
        "        topics_dict[\"body\"].append(submission.selftext)\n",
        "\n",
        "topics_data = pd.DataFrame(topics_dict)\n",
        "topics_data\n",
        "topics_data.to_csv('reddit_Df.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_PcsL8O4HuU"
      },
      "source": [
        "# Using a list instead of a dict to store the features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYQLN509f0N4"
      },
      "source": [
        "#create a list of all the subreddits you want to scrape \n",
        "subreddit_list=['subreddit_1',\n",
        "                'subreddit_2',\n",
        "               ]\n",
        "                \n",
        "#create a list of all features you want to gather from the API\n",
        "author_list = []\n",
        "id_list = []\n",
        "self_text_list = []\n",
        "num_comments_list = []\n",
        "score_list = []\n",
        "title_list = []\n",
        "upvote_ratio_list = []\n",
        "created_at_list=[]\n",
        "pinned_list=[]\n",
        "total_awards_received=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiplZmF1mOzv"
      },
      "source": [
        "#Script will iterate through the list of 10 subreddit names, scraping & storing specific attributes.\n",
        "for subred in subreddit_list:\n",
        "    subreddit=reddit.subreddit(subred)\n",
        "    new_post=subreddit.hot(limit=1000)\n",
        "\n",
        "\n",
        "    for sub in new_post:\n",
        "        author_list.append(sub.author)\n",
        "        id_list.append(sub.id)\n",
        "        self_text_list.append(sub.sel_text)\n",
        "        num_comments_list.append(sub.nums_comments)\n",
        "        score_list.append(sub.score)\n",
        "        title_list.append(sub.title)\n",
        "        upvote_ratio_list.append(sub.upvote_ratio)\n",
        "        created_at_list.append(sub.created)\n",
        "        pinned_list.append(sub.pinned)\n",
        "        total_awards_received.append(sub.total_awards_received)\n",
        "        \n",
        "    print(subred, 'completed; ', end='')\n",
        "    print('total', len(author_list), 'posts has been scraped')\n",
        "\n",
        "#creating the df\n",
        "a=pd.DataFrame({\n",
        "'ID':id_list,\n",
        "'Author':author_list,\n",
        "'Title' :title_list,\n",
        "'Count_of_Comments':num_comments_list,\n",
        "'Upvote_Count':score_list,\n",
        "'text':self_text_list,\n",
        "'Upvote_ratio':upvote_ratio_list,\n",
        "'created_at':created_at_list,\n",
        "'pinnned':pinned_list,\n",
        "'total_awards_received': total_awards_received,\n",
        "})\n",
        "\n",
        "reddit_data=a.to_csv('reddit_data.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A077EO7wpzEM"
      },
      "source": [
        "#creating the df\n",
        "a=pd.DataFrame({\n",
        "    'ID':id_list,\n",
        "    'Author':author_list,\n",
        "    'Title' :title_list,\n",
        "    'Count_of_Comments':num_comments_list,\n",
        "    'Upvote_Count':score_list,\n",
        "    'text':self_text_list,\n",
        "    'Upvote_ratio':upvote_ratio_list,\n",
        "    'created_at':created_at_list,\n",
        "    'pinnned':pinned_list,\n",
        "    'total_awards_received': total_awards_received,\n",
        "})\n",
        "\n",
        "reddit_data=a.to_csv('reddit_data.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwVcntKLuAch"
      },
      "source": [
        ""
      ]
    }
  ]
}